{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b422af",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Evaluación de Modelos de Machine Learning usando Validación Cruzada\n",
    "\n",
    "Este script implementa una evaluación completa de modelos de machine learning\n",
    "utilizando validación cruzada. Incluye:\n",
    "- Carga y preparación de datos\n",
    "- Definición de múltiples modelos\n",
    "- Implementación de validación cruzada\n",
    "- Evaluación con múltiples métricas\n",
    "- Visualización de resultados\n",
    "\n",
    "Autor: [Tu nombre]\n",
    "Fecha: [Fecha actual]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b9ed15",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PASO 1: IMPORTAR LAS BIBLIOTECAS NECESARIAS\n",
    "# =============================================================================\n",
    "\n",
    "# Importar bibliotecas para análisis de datos y manipulación\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Importar bibliotecas de scikit-learn para machine learning\n",
    "from sklearn.datasets import load_iris, load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, KFold, StratifiedKFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "# Importar algoritmos de clasificación\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Importar bibliotecas para visualización\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Configurar el estilo de las gráficas\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Configurar para mostrar todas las columnas en pandas\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)\n",
    "\n",
    "print(\"✓ Bibliotecas importadas exitosamente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e151c2b",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PASO 2: CARGAR LOS DATOS\n",
    "# =============================================================================\n",
    "\n",
    "def cargar_datos():\n",
    "    \"\"\"\n",
    "    Carga el dataset de iris y prepara los datos para el análisis.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (X, y, feature_names, target_names, df_iris)\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"PASO 2: CARGAR LOS DATOS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Cargar el dataset de iris\n",
    "    iris = load_iris()\n",
    "    \n",
    "    # Extraer las características (features) y las etiquetas (target)\n",
    "    X = iris.data  # Matriz de características\n",
    "    y = iris.target  # Vector de etiquetas\n",
    "    \n",
    "    # Obtener los nombres de las características y clases\n",
    "    feature_names = iris.feature_names\n",
    "    target_names = iris.target_names\n",
    "    \n",
    "    # Crear un DataFrame para mejor visualización\n",
    "    df_iris = pd.DataFrame(X, columns=feature_names)\n",
    "    df_iris['target'] = y\n",
    "    df_iris['target_name'] = [target_names[i] for i in y]\n",
    "    \n",
    "    # Mostrar información básica del dataset\n",
    "    print(\"=== INFORMACIÓN DEL DATASET IRIS ===\")\n",
    "    print(f\"Forma de los datos: {X.shape}\")\n",
    "    print(f\"Número de características: {X.shape[1]}\")\n",
    "    print(f\"Número de muestras: {X.shape[0]}\")\n",
    "    print(f\"Número de clases: {len(np.unique(y))}\")\n",
    "    print(f\"Clases: {target_names}\")\n",
    "    print(f\"Características: {feature_names}\")\n",
    "    \n",
    "    # Mostrar las primeras filas del dataset\n",
    "    print(\"\\n=== PRIMERAS 5 FILAS DEL DATASET ===\")\n",
    "    print(df_iris.head())\n",
    "    \n",
    "    # Mostrar estadísticas descriptivas\n",
    "    print(\"\\n=== ESTADÍSTICAS DESCRIPTIVAS ===\")\n",
    "    print(df_iris.describe())\n",
    "    \n",
    "    # Mostrar distribución de clases\n",
    "    print(\"\\n=== DISTRIBUCIÓN DE CLASES ===\")\n",
    "    class_distribution = df_iris['target_name'].value_counts()\n",
    "    print(class_distribution)\n",
    "    \n",
    "    return X, y, feature_names, target_names, df_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd2ec98",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PASO 3: VISUALIZACIÓN EXPLORATORIA DE LOS DATOS\n",
    "# =============================================================================\n",
    "\n",
    "def visualizar_datos(df_iris, feature_names, target_names):\n",
    "    \"\"\"\n",
    "    Crea visualizaciones exploratorias de los datos.\n",
    "    \n",
    "    Args:\n",
    "        df_iris (DataFrame): DataFrame con los datos\n",
    "        feature_names (list): Lista de nombres de características\n",
    "        target_names (list): Lista de nombres de clases\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"PASO 3: VISUALIZACIÓN EXPLORATORIA DE LOS DATOS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Crear una figura con múltiples subplots para visualizar los datos\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
    "    fig.suptitle('Análisis Exploratorio del Dataset Iris', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Distribución de clases\n",
    "    class_distribution = df_iris['target_name'].value_counts()\n",
    "    axes[0, 0].pie(class_distribution.values, labels=class_distribution.index, \n",
    "                   autopct='%1.1f%%', startangle=90)\n",
    "    axes[0, 0].set_title('Distribución de Clases')\n",
    "    \n",
    "    # 2. Histograma de una característica (longitud del sépalo)\n",
    "    axes[0, 1].hist(df_iris[df_iris['target_name'] == 'setosa']['sepal length (cm)'], \n",
    "                    alpha=0.7, label='Setosa', bins=15)\n",
    "    axes[0, 1].hist(df_iris[df_iris['target_name'] == 'versicolor']['sepal length (cm)'], \n",
    "                    alpha=0.7, label='Versicolor', bins=15)\n",
    "    axes[0, 1].hist(df_iris[df_iris['target_name'] == 'virginica']['sepal length (cm)'], \n",
    "                    alpha=0.7, label='Virginica', bins=15)\n",
    "    axes[0, 1].set_xlabel('Longitud del Sépalo (cm)')\n",
    "    axes[0, 1].set_ylabel('Frecuencia')\n",
    "    axes[0, 1].set_title('Distribución de Longitud del Sépalo por Clase')\n",
    "    axes[0, 1].legend()\n",
    "    \n",
    "    # 3. Scatter plot de dos características\n",
    "    scatter = axes[1, 0].scatter(df_iris['sepal length (cm)'], df_iris['sepal width (cm)'], \n",
    "                                c=df_iris['target'], cmap='viridis', alpha=0.7)\n",
    "    axes[1, 0].set_xlabel('Longitud del Sépalo (cm)')\n",
    "    axes[1, 0].set_ylabel('Ancho del Sépalo (cm)')\n",
    "    axes[1, 0].set_title('Longitud vs Ancho del Sépalo')\n",
    "    plt.colorbar(scatter, ax=axes[1, 0])\n",
    "    \n",
    "    # 4. Matriz de correlación\n",
    "    correlation_matrix = df_iris[feature_names].corr()\n",
    "    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, \n",
    "                square=True, ax=axes[1, 1])\n",
    "    axes[1, 1].set_title('Matriz de Correlación')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Mostrar la matriz de correlación en formato numérico\n",
    "    print(\"\\n=== MATRIZ DE CORRELACIÓN ===\")\n",
    "    print(correlation_matrix.round(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "047a382f",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PASO 4: PREPROCESAMIENTO DE DATOS\n",
    "# =============================================================================\n",
    "\n",
    "def preprocesar_datos(X, feature_names):\n",
    "    \"\"\"\n",
    "    Aplica técnicas de preprocesamiento a los datos.\n",
    "    \n",
    "    Args:\n",
    "        X (array): Matriz de características\n",
    "        feature_names (list): Lista de nombres de características\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (X_scaled, df_scaled)\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"PASO 4: PREPROCESAMIENTO DE DATOS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Crear una instancia del escalador estándar\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    # Aplicar escalado a las características\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # Crear un DataFrame con los datos escalados\n",
    "    df_scaled = pd.DataFrame(X_scaled, columns=feature_names)\n",
    "    \n",
    "    # Mostrar estadísticas antes y después del escalado\n",
    "    print(\"=== COMPARACIÓN ANTES Y DESPUÉS DEL ESCALADO ===\")\n",
    "    print(\"\\nEstadísticas antes del escalado:\")\n",
    "    df_original = pd.DataFrame(X, columns=feature_names)\n",
    "    print(df_original.describe().round(3))\n",
    "    \n",
    "    print(\"\\nEstadísticas después del escalado:\")\n",
    "    print(df_scaled.describe().round(3))\n",
    "    \n",
    "    # Verificar que el escalado funcionó correctamente\n",
    "    print(\"\\n=== VERIFICACIÓN DEL ESCALADO ===\")\n",
    "    print(f\"Media de características escaladas: {df_scaled.mean().round(6).to_dict()}\")\n",
    "    print(f\"Desviación estándar de características escaladas: {df_scaled.std().round(6).to_dict()}\")\n",
    "    \n",
    "    return X_scaled, df_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65de6b66",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PASO 5: DEFINIR LOS MODELOS A EVALUAR\n",
    "# =============================================================================\n",
    "\n",
    "def definir_modelos():\n",
    "    \"\"\"\n",
    "    Define múltiples modelos de machine learning con diferentes configuraciones.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Diccionario con los modelos definidos\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"PASO 5: DEFINIR LOS MODELOS A EVALUAR\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Definir los modelos que queremos evaluar\n",
    "    # Cada modelo se define con sus hiperparámetros específicos\n",
    "    models = {\n",
    "        'KNN (k=3)': KNeighborsClassifier(n_neighbors=3),\n",
    "        'KNN (k=5)': KNeighborsClassifier(n_neighbors=5),\n",
    "        'KNN (k=7)': KNeighborsClassifier(n_neighbors=7),\n",
    "        'Decision Tree (max_depth=3)': DecisionTreeClassifier(max_depth=3, random_state=42),\n",
    "        'Decision Tree (max_depth=5)': DecisionTreeClassifier(max_depth=5, random_state=42),\n",
    "        'Random Forest (n_estimators=50)': RandomForestClassifier(n_estimators=50, random_state=42),\n",
    "        'Random Forest (n_estimators=100)': RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "        'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "        'SVM (linear)': SVC(kernel='linear', random_state=42),\n",
    "        'SVM (rbf)': SVC(kernel='rbf', random_state=42)\n",
    "    }\n",
    "    \n",
    "    # Mostrar información sobre los modelos definidos\n",
    "    print(\"=== MODELOS DEFINIDOS ===\")\n",
    "    for name, model in models.items():\n",
    "        print(f\"{name}: {type(model).__name__}\")\n",
    "    \n",
    "    print(f\"\\nTotal de modelos a evaluar: {len(models)}\")\n",
    "    \n",
    "    return models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42161923",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PASO 6: DEFINIR LA ESTRATEGIA DE VALIDACIÓN CRUZADA\n",
    "# =============================================================================\n",
    "\n",
    "def definir_estrategias_cv():\n",
    "    \"\"\"\n",
    "    Define diferentes estrategias de validación cruzada.\n",
    "    \n",
    "    Returns:\n",
    "        tuple: (cv_strategies, main_cv)\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"PASO 6: DEFINIR LA ESTRATEGIA DE VALIDACIÓN CRUZADA\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Definir diferentes estrategias de validación cruzada\n",
    "    cv_strategies = {\n",
    "        'KFold (k=5)': KFold(n_splits=5, shuffle=True, random_state=42),\n",
    "        'KFold (k=10)': KFold(n_splits=10, shuffle=True, random_state=42),\n",
    "        'StratifiedKFold (k=5)': StratifiedKFold(n_splits=5, shuffle=True, random_state=42),\n",
    "        'StratifiedKFold (k=10)': StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
    "    }\n",
    "    \n",
    "    # Mostrar información sobre las estrategias de validación cruzada\n",
    "    print(\"=== ESTRATEGIAS DE VALIDACIÓN CRUZADA ===\")\n",
    "    for name, cv in cv_strategies.items():\n",
    "        print(f\"{name}: {type(cv).__name__}\")\n",
    "    \n",
    "    # Usaremos StratifiedKFold con 5 pliegues como estrategia principal\n",
    "    # ya que es más apropiada para problemas de clasificación con clases desbalanceadas\n",
    "    main_cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    print(f\"\\nEstrategia principal seleccionada: StratifiedKFold con 5 pliegues\")\n",
    "    \n",
    "    return cv_strategies, main_cv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7609c33",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PASO 7: DEFINIR LAS MÉTRICAS DE EVALUACIÓN\n",
    "# =============================================================================\n",
    "\n",
    "def definir_metricas():\n",
    "    \"\"\"\n",
    "    Define múltiples métricas para evaluar el rendimiento de los modelos.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Diccionario con las métricas definidas\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"PASO 7: DEFINIR LAS MÉTRICAS DE EVALUACIÓN\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Definir las métricas de evaluación que queremos usar\n",
    "    metrics = {\n",
    "        'accuracy': 'accuracy',\n",
    "        'precision_macro': 'precision_macro',\n",
    "        'recall_macro': 'recall_macro',\n",
    "        'f1_macro': 'f1_macro'\n",
    "    }\n",
    "    \n",
    "    # Mostrar información sobre las métricas\n",
    "    print(\"=== MÉTRICAS DE EVALUACIÓN ===\")\n",
    "    metric_descriptions = {\n",
    "        'accuracy': 'Precisión general (proporción de predicciones correctas)',\n",
    "        'precision_macro': 'Precisión macro (promedio de precisión por clase)',\n",
    "        'recall_macro': 'Recall macro (promedio de recall por clase)',\n",
    "        'f1_macro': 'F1-score macro (promedio armónico de precisión y recall por clase)'\n",
    "    }\n",
    "    \n",
    "    for metric, description in metric_descriptions.items():\n",
    "        print(f\"{metric}: {description}\")\n",
    "    \n",
    "    print(f\"\\nTotal de métricas a evaluar: {len(metrics)}\")\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "247ae847",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PASO 8: REALIZAR VALIDACIÓN CRUZADA CON MÚLTIPLES MÉTRICAS\n",
    "# =============================================================================\n",
    "\n",
    "def realizar_validacion_cruzada(models, X_scaled, y, main_cv, metrics):\n",
    "    \"\"\"\n",
    "    Implementa la validación cruzada para todos los modelos usando todas las métricas definidas.\n",
    "    \n",
    "    Args:\n",
    "        models (dict): Diccionario con los modelos a evaluar\n",
    "        X_scaled (array): Datos escalados\n",
    "        y (array): Etiquetas\n",
    "        main_cv: Estrategia de validación cruzada\n",
    "        metrics (dict): Diccionario con las métricas\n",
    "    \n",
    "    Returns:\n",
    "        dict: Diccionario con todos los resultados\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"PASO 8: REALIZAR VALIDACIÓN CRUZADA CON MÚLTIPLES MÉTRICAS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Diccionario para almacenar todos los resultados\n",
    "    results = {}\n",
    "    \n",
    "    # Realizar validación cruzada para cada modelo y cada métrica\n",
    "    print(\"=== REALIZANDO VALIDACIÓN CRUZADA ===\")\n",
    "    print(\"Procesando modelos...\")\n",
    "    \n",
    "    for model_name, model in models.items():\n",
    "        print(f\"\\nEvaluando {model_name}...\")\n",
    "        results[model_name] = {}\n",
    "        \n",
    "        for metric_name, metric in metrics.items():\n",
    "            # Realizar validación cruzada\n",
    "            scores = cross_val_score(model, X_scaled, y, cv=main_cv, scoring=metric)\n",
    "            \n",
    "            # Almacenar resultados\n",
    "            results[model_name][metric_name] = {\n",
    "                'scores': scores,\n",
    "                'mean': scores.mean(),\n",
    "                'std': scores.std(),\n",
    "                'min': scores.min(),\n",
    "                'max': scores.max()\n",
    "            }\n",
    "            \n",
    "            print(f\"  {metric_name}: {scores.mean():.4f} (+/- {scores.std() * 2:.4f})\")\n",
    "    \n",
    "    print(\"\\n¡Validación cruzada completada exitosamente!\")\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d2d33f2",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PASO 9: CALCULAR Y MOSTRAR LAS PUNTUACIONES MEDIAS\n",
    "# =============================================================================\n",
    "\n",
    "def calcular_puntuaciones_medias(results, models, metrics):\n",
    "    \"\"\"\n",
    "    Calcula las puntuaciones medias para cada modelo y métrica.\n",
    "    \n",
    "    Args:\n",
    "        results (dict): Resultados de la validación cruzada\n",
    "        models (dict): Modelos evaluados\n",
    "        metrics (dict): Métricas utilizadas\n",
    "    \n",
    "    Returns:\n",
    "        DataFrame: DataFrame con los resultados organizados\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"PASO 9: CALCULAR Y MOSTRAR LAS PUNTUACIONES MEDIAS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Crear un DataFrame con los resultados de todas las métricas\n",
    "    results_df = pd.DataFrame()\n",
    "    \n",
    "    for model_name in models.keys():\n",
    "        for metric_name in metrics.keys():\n",
    "            mean_score = results[model_name][metric_name]['mean']\n",
    "            std_score = results[model_name][metric_name]['std']\n",
    "            \n",
    "            # Agregar fila al DataFrame\n",
    "            new_row = pd.DataFrame({\n",
    "                'Modelo': [model_name],\n",
    "                'Métrica': [metric_name],\n",
    "                'Puntuación Media': [mean_score],\n",
    "                'Desviación Estándar': [std_score],\n",
    "                'Intervalo de Confianza': [f\"{mean_score:.4f} ± {std_score*2:.4f}\"]\n",
    "            })\n",
    "            \n",
    "            results_df = pd.concat([results_df, new_row], ignore_index=True)\n",
    "    \n",
    "    # Mostrar resultados completos\n",
    "    print(\"=== RESULTADOS COMPLETOS DE VALIDACIÓN CRUZADA ===\")\n",
    "    print(results_df.round(4))\n",
    "    \n",
    "    # Crear una tabla pivot para mejor visualización\n",
    "    pivot_results = results_df.pivot(index='Modelo', columns='Métrica', values='Puntuación Media')\n",
    "    \n",
    "    print(\"\\n=== TABLA PIVOT DE PUNTUACIONES MEDIAS ===\")\n",
    "    print(pivot_results.round(4))\n",
    "    \n",
    "    return results_df, pivot_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d97ca9d",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PASO 10: VISUALIZAR LOS RESULTADOS\n",
    "# =============================================================================\n",
    "\n",
    "def visualizar_resultados(results_df, pivot_results, models, results):\n",
    "    \"\"\"\n",
    "    Crea visualizaciones para comparar el rendimiento de los diferentes modelos.\n",
    "    \n",
    "    Args:\n",
    "        results_df (DataFrame): DataFrame con resultados completos\n",
    "        pivot_results (DataFrame): Tabla pivot con resultados\n",
    "        models (dict): Modelos evaluados\n",
    "        results (dict): Resultados originales de validación cruzada\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"PASO 10: VISUALIZAR LOS RESULTADOS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Crear visualizaciones de los resultados\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(20, 16))\n",
    "    fig.suptitle('Comparación de Rendimiento de Modelos - Validación Cruzada', \n",
    "                 fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # 1. Gráfico de barras para accuracy\n",
    "    accuracy_data = results_df[results_df['Métrica'] == 'accuracy']\n",
    "    bars1 = axes[0, 0].bar(range(len(accuracy_data)), accuracy_data['Puntuación Media'], \n",
    "                           yerr=accuracy_data['Desviación Estándar'], capsize=5, alpha=0.7)\n",
    "    axes[0, 0].set_title('Accuracy por Modelo')\n",
    "    axes[0, 0].set_ylabel('Accuracy')\n",
    "    axes[0, 0].set_xticks(range(len(accuracy_data)))\n",
    "    axes[0, 0].set_xticklabels(accuracy_data['Modelo'], rotation=45, ha='right')\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 2. Gráfico de barras para F1-score\n",
    "    f1_data = results_df[results_df['Métrica'] == 'f1_macro']\n",
    "    bars2 = axes[0, 1].bar(range(len(f1_data)), f1_data['Puntuación Media'], \n",
    "                           yerr=f1_data['Desviación Estándar'], capsize=5, alpha=0.7, color='orange')\n",
    "    axes[0, 1].set_title('F1-Score Macro por Modelo')\n",
    "    axes[0, 1].set_ylabel('F1-Score Macro')\n",
    "    axes[0, 1].set_xticks(range(len(f1_data)))\n",
    "    axes[0, 1].set_xticklabels(f1_data['Modelo'], rotation=45, ha='right')\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    # 3. Heatmap de todas las métricas\n",
    "    heatmap_data = pivot_results\n",
    "    sns.heatmap(heatmap_data, annot=True, cmap='RdYlGn', center=0.8, \n",
    "                square=True, ax=axes[1, 0], fmt='.3f')\n",
    "    axes[1, 0].set_title('Heatmap de Rendimiento - Todas las Métricas')\n",
    "    axes[1, 0].set_xlabel('Métricas')\n",
    "    axes[1, 0].set_ylabel('Modelos')\n",
    "    \n",
    "    # 4. Box plot de las puntuaciones de accuracy\n",
    "    accuracy_scores = []\n",
    "    model_names = []\n",
    "    for model_name in models.keys():\n",
    "        scores = results[model_name]['accuracy']['scores']\n",
    "        accuracy_scores.extend(scores)\n",
    "        model_names.extend([model_name] * len(scores))\n",
    "    \n",
    "    box_data = pd.DataFrame({'Modelo': model_names, 'Accuracy': accuracy_scores})\n",
    "    sns.boxplot(data=box_data, x='Modelo', y='Accuracy', ax=axes[1, 1])\n",
    "    axes[1, 1].set_title('Distribución de Accuracy por Modelo')\n",
    "    axes[1, 1].set_xticklabels(axes[1, 1].get_xticklabels(), rotation=45, ha='right')\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd82e5c7",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PASO 11: ANÁLISIS DETALLADO DEL MEJOR MODELO\n",
    "# =============================================================================\n",
    "\n",
    "def analizar_mejor_modelo(results, models, X_scaled, y, target_names):\n",
    "    \"\"\"\n",
    "    Identifica el mejor modelo y realiza un análisis detallado.\n",
    "    \n",
    "    Args:\n",
    "        results (dict): Resultados de la validación cruzada\n",
    "        models (dict): Modelos evaluados\n",
    "        X_scaled (array): Datos escalados\n",
    "        y (array): Etiquetas\n",
    "        target_names (list): Nombres de las clases\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"PASO 11: ANÁLISIS DETALLADO DEL MEJOR MODELO\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Crear tabla pivot para identificar el mejor modelo\n",
    "    results_df = pd.DataFrame()\n",
    "    for model_name in models.keys():\n",
    "        for metric_name in ['accuracy', 'f1_macro', 'precision_macro', 'recall_macro']:\n",
    "            mean_score = results[model_name][metric_name]['mean']\n",
    "            new_row = pd.DataFrame({\n",
    "                'Modelo': [model_name],\n",
    "                'Métrica': [metric_name],\n",
    "                'Puntuación Media': [mean_score]\n",
    "            })\n",
    "            results_df = pd.concat([results_df, new_row], ignore_index=True)\n",
    "    \n",
    "    pivot_results = results_df.pivot(index='Modelo', columns='Métrica', values='Puntuación Media')\n",
    "    \n",
    "    # Identificar el mejor modelo según accuracy\n",
    "    best_model_name = pivot_results['accuracy'].idxmax()\n",
    "    best_model_score = pivot_results.loc[best_model_name, 'accuracy']\n",
    "    \n",
    "    print(\"=== ANÁLISIS DEL MEJOR MODELO ===\")\n",
    "    print(f\"Mejor modelo según accuracy: {best_model_name}\")\n",
    "    print(f\"Puntuación de accuracy: {best_model_score:.4f}\")\n",
    "    \n",
    "    # Mostrar todas las métricas del mejor modelo\n",
    "    print(f\"\\nRendimiento completo del mejor modelo:\")\n",
    "    best_model_metrics = pivot_results.loc[best_model_name]\n",
    "    for metric, score in best_model_metrics.items():\n",
    "        print(f\"  {metric}: {score:.4f}\")\n",
    "    \n",
    "    # Entrenar el mejor modelo en todo el dataset para análisis detallado\n",
    "    best_model = models[best_model_name]\n",
    "    best_model.fit(X_scaled, y)\n",
    "    \n",
    "    # Realizar predicciones\n",
    "    y_pred = best_model.predict(X_scaled)\n",
    "    \n",
    "    # Mostrar reporte de clasificación detallado\n",
    "    print(f\"\\n=== REPORTE DE CLASIFICACIÓN DETALLADO ===\")\n",
    "    print(f\"Modelo: {best_model_name}\")\n",
    "    print(classification_report(y, y_pred, target_names=target_names))\n",
    "    \n",
    "    # Mostrar matriz de confusión\n",
    "    print(f\"\\n=== MATRIZ DE CONFUSIÓN ===\")\n",
    "    cm = confusion_matrix(y, y_pred)\n",
    "    print(cm)\n",
    "    \n",
    "    # Visualizar matriz de confusión\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=target_names, yticklabels=target_names)\n",
    "    plt.title(f'Matriz de Confusión - {best_model_name}')\n",
    "    plt.ylabel('Etiqueta Real')\n",
    "    plt.xlabel('Etiqueta Predicha')\n",
    "    plt.show()\n",
    "    \n",
    "    return best_model_name, best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d938545",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PASO 12: COMPARACIÓN DE ESTRATEGIAS DE VALIDACIÓN CRUZADA\n",
    "# =============================================================================\n",
    "\n",
    "def comparar_estrategias_cv(cv_strategies, best_model, X_scaled, y):\n",
    "    \"\"\"\n",
    "    Compara el rendimiento usando diferentes estrategias de validación cruzada.\n",
    "    \n",
    "    Args:\n",
    "        cv_strategies (dict): Diferentes estrategias de validación cruzada\n",
    "        best_model: El mejor modelo identificado\n",
    "        X_scaled (array): Datos escalados\n",
    "        y (array): Etiquetas\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"PASO 12: COMPARACIÓN DE ESTRATEGIAS DE VALIDACIÓN CRUZADA\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Comparar diferentes estrategias de validación cruzada\n",
    "    print(\"=== COMPARACIÓN DE ESTRATEGIAS DE VALIDACIÓN CRUZADA ===\")\n",
    "    \n",
    "    # Usar el mejor modelo para la comparación\n",
    "    cv_comparison = {}\n",
    "    \n",
    "    for cv_name, cv_strategy in cv_strategies.items():\n",
    "        scores = cross_val_score(best_model, X_scaled, y, cv=cv_strategy, scoring='accuracy')\n",
    "        cv_comparison[cv_name] = {\n",
    "            'scores': scores,\n",
    "            'mean': scores.mean(),\n",
    "            'std': scores.std()\n",
    "        }\n",
    "        print(f\"{cv_name}: {scores.mean():.4f} (+/- {scores.std() * 2:.4f})\")\n",
    "    \n",
    "    # Visualizar comparación de estrategias CV\n",
    "    cv_names = list(cv_comparison.keys())\n",
    "    cv_means = [cv_comparison[name]['mean'] for name in cv_names]\n",
    "    cv_stds = [cv_comparison[name]['std'] for name in cv_names]\n",
    "    \n",
    "    plt.figure(figsize=(10, 6))\n",
    "    bars = plt.bar(cv_names, cv_means, yerr=cv_stds, capsize=5, alpha=0.7, color='skyblue')\n",
    "    plt.title('Comparación de Estrategias de Validación Cruzada')\n",
    "    plt.ylabel('Accuracy Media')\n",
    "    plt.xlabel('Estrategia de Validación Cruzada')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Agregar valores en las barras\n",
    "    for bar, mean, std in zip(bars, cv_means, cv_stds):\n",
    "        height = bar.get_height()\n",
    "        plt.text(bar.get_x() + bar.get_width()/2., height + std + 0.01,\n",
    "                 f'{mean:.4f}\\n±{std*2:.4f}', ha='center', va='bottom', fontsize=9)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d9173d",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# PASO 13: RESUMEN Y CONCLUSIONES\n",
    "# =============================================================================\n",
    "\n",
    "def generar_resumen(results, models, metrics, best_model_name, main_cv):\n",
    "    \"\"\"\n",
    "    Genera un resumen ejecutivo de los hallazgos principales.\n",
    "    \n",
    "    Args:\n",
    "        results (dict): Resultados de la validación cruzada\n",
    "        models (dict): Modelos evaluados\n",
    "        metrics (dict): Métricas utilizadas\n",
    "        best_model_name (str): Nombre del mejor modelo\n",
    "        main_cv: Estrategia de validación cruzada principal\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"PASO 13: RESUMEN Y CONCLUSIONES\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Crear tabla pivot para el resumen\n",
    "    results_df = pd.DataFrame()\n",
    "    for model_name in models.keys():\n",
    "        for metric_name in metrics.keys():\n",
    "            mean_score = results[model_name][metric_name]['mean']\n",
    "            new_row = pd.DataFrame({\n",
    "                'Modelo': [model_name],\n",
    "                'Métrica': [metric_name],\n",
    "                'Puntuación Media': [mean_score]\n",
    "            })\n",
    "            results_df = pd.concat([results_df, new_row], ignore_index=True)\n",
    "    \n",
    "    pivot_results = results_df.pivot(index='Modelo', columns='Métrica', values='Puntuación Media')\n",
    "    \n",
    "    # Crear un resumen ejecutivo de los resultados\n",
    "    print(\"=== RESUMEN EJECUTIVO ===\")\n",
    "    print(\"\\n1. MEJORES MODELOS POR MÉTRICA:\")\n",
    "    for metric in metrics.keys():\n",
    "        best_for_metric = pivot_results[metric].idxmax()\n",
    "        best_score = pivot_results.loc[best_for_metric, metric]\n",
    "        print(f\"   {metric}: {best_for_metric} ({best_score:.4f})\")\n",
    "    \n",
    "    print(\"\\n2. TOP 5 MODELOS POR ACCURACY:\")\n",
    "    top_5_accuracy = pivot_results['accuracy'].sort_values(ascending=False).head(5)\n",
    "    for i, (model, score) in enumerate(top_5_accuracy.items(), 1):\n",
    "        print(f\"   {i}. {model}: {score:.4f}\")\n",
    "    \n",
    "    print(\"\\n3. ESTADÍSTICAS GENERALES:\")\n",
    "    print(f\"   Número total de modelos evaluados: {len(models)}\")\n",
    "    print(f\"   Número de pliegues en validación cruzada: {main_cv.n_splits}\")\n",
    "    print(f\"   Métricas evaluadas: {list(metrics.keys())}\")\n",
    "    print(f\"   Accuracy promedio de todos los modelos: {pivot_results['accuracy'].mean():.4f}\")\n",
    "    print(f\"   Accuracy más alta: {pivot_results['accuracy'].max():.4f}\")\n",
    "    print(f\"   Accuracy más baja: {pivot_results['accuracy'].min():.4f}\")\n",
    "    \n",
    "    print(\"\\n4. RECOMENDACIONES:\")\n",
    "    print(f\"   - El mejor modelo general es: {best_model_name}\")\n",
    "    print(f\"   - Estrategia de validación cruzada recomendada: StratifiedKFold con 5 pliegues\")\n",
    "    print(f\"   - Los datos están bien balanceados, por lo que accuracy es una métrica apropiada\")\n",
    "    print(f\"   - El escalado de características mejoró el rendimiento de los modelos\")\n",
    "    \n",
    "    # Crear una tabla final de recomendaciones\n",
    "    best_model_score = pivot_results.loc[best_model_name, 'accuracy']\n",
    "    recommendations_df = pd.DataFrame({\n",
    "        'Aspecto': ['Mejor Modelo', 'Accuracy', 'Estrategia CV', 'Métricas Evaluadas', 'Preprocesamiento'],\n",
    "        'Recomendación': [\n",
    "            best_model_name,\n",
    "            f\"{best_model_score:.4f}\",\n",
    "            'StratifiedKFold (k=5)',\n",
    "            ', '.join(metrics.keys()),\n",
    "            'StandardScaler'\n",
    "        ]\n",
    "    })\n",
    "    \n",
    "    print(\"\\n5. TABLA DE RECOMENDACIONES:\")\n",
    "    print(recommendations_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc32835",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# RESUMEN EJECUTIVO - EJECUTAR TODO EL ANÁLISIS\n",
    "# =============================================================================\n",
    "\n",
    "# Ejecutar todo el análisis paso a paso\n",
    "print(\"=\"*80)\n",
    "print(\"EVALUACIÓN DE MODELOS DE MACHINE LEARNING USANDO VALIDACIÓN CRUZADA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Paso 2: Cargar los datos\n",
    "X, y, feature_names, target_names, df_iris = cargar_datos()\n",
    "\n",
    "# Paso 3: Visualización exploratoria\n",
    "visualizar_datos(df_iris, feature_names, target_names)\n",
    "\n",
    "# Paso 4: Preprocesamiento\n",
    "X_scaled, df_scaled = preprocesar_datos(X, feature_names)\n",
    "\n",
    "# Paso 5: Definir modelos\n",
    "models = definir_modelos()\n",
    "\n",
    "# Paso 6: Definir estrategias de validación cruzada\n",
    "cv_strategies, main_cv = definir_estrategias_cv()\n",
    "\n",
    "# Paso 7: Definir métricas\n",
    "metrics = definir_metricas()\n",
    "\n",
    "# Paso 8: Realizar validación cruzada\n",
    "results = realizar_validacion_cruzada(models, X_scaled, y, main_cv, metrics)\n",
    "\n",
    "# Paso 9: Calcular puntuaciones medias\n",
    "results_df, pivot_results = calcular_puntuaciones_medias(results, models, metrics)\n",
    "\n",
    "# Paso 10: Visualizar resultados\n",
    "visualizar_resultados(results_df, pivot_results, models, results)\n",
    "\n",
    "# Paso 11: Análisis del mejor modelo\n",
    "best_model_name, best_model = analizar_mejor_modelo(results, models, X_scaled, y, target_names)\n",
    "\n",
    "# Paso 12: Comparar estrategias de validación cruzada\n",
    "comparar_estrategias_cv(cv_strategies, best_model, X_scaled, y)\n",
    "\n",
    "# Paso 13: Generar resumen\n",
    "generar_resumen(results, models, metrics, best_model_name, main_cv)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"¡EVALUACIÓN COMPLETADA EXITOSAMENTE!\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# =============================================================================\n",
    "# FUNCIÓN PRINCIPAL\n",
    "# =============================================================================\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Función principal que ejecuta todo el proceso de evaluación de modelos.\n",
    "    \"\"\"\n",
    "    print(\"=\"*80)\n",
    "    print(\"EVALUACIÓN DE MODELOS DE MACHINE LEARNING USANDO VALIDACIÓN CRUZADA\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    try:\n",
    "        # Paso 1: Las bibliotecas ya están importadas al inicio del script\n",
    "        \n",
    "        # Paso 2: Cargar los datos\n",
    "        X, y, feature_names, target_names, df_iris = cargar_datos()\n",
    "        \n",
    "        # Paso 3: Visualización exploratoria\n",
    "        visualizar_datos(df_iris, feature_names, target_names)\n",
    "        \n",
    "        # Paso 4: Preprocesamiento\n",
    "        X_scaled, df_scaled = preprocesar_datos(X, feature_names)\n",
    "        \n",
    "        # Paso 5: Definir modelos\n",
    "        models = definir_modelos()\n",
    "        \n",
    "        # Paso 6: Definir estrategias de validación cruzada\n",
    "        cv_strategies, main_cv = definir_estrategias_cv()\n",
    "        \n",
    "        # Paso 7: Definir métricas\n",
    "        metrics = definir_metricas()\n",
    "        \n",
    "        # Paso 8: Realizar validación cruzada\n",
    "        results = realizar_validacion_cruzada(models, X_scaled, y, main_cv, metrics)\n",
    "        \n",
    "        # Paso 9: Calcular puntuaciones medias\n",
    "        results_df, pivot_results = calcular_puntuaciones_medias(results, models, metrics)\n",
    "        \n",
    "        # Paso 10: Visualizar resultados\n",
    "        visualizar_resultados(results_df, pivot_results, models, results)\n",
    "        \n",
    "        # Paso 11: Análisis del mejor modelo\n",
    "        best_model_name, best_model = analizar_mejor_modelo(results, models, X_scaled, y, target_names)\n",
    "        \n",
    "        # Paso 12: Comparar estrategias de validación cruzada\n",
    "        comparar_estrategias_cv(cv_strategies, best_model, X_scaled, y)\n",
    "        \n",
    "        # Paso 13: Generar resumen\n",
    "        generar_resumen(results, models, metrics, best_model_name, main_cv)\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"¡EVALUACIÓN COMPLETADA EXITOSAMENTE!\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\nError durante la ejecución: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "# =============================================================================\n",
    "# EJECUTAR EL SCRIPT\n",
    "# =============================================================================\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# -*- coding: utf-8 -*-",
   "executable": "/usr/bin/env python3",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
